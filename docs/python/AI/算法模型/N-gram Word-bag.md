# N-gram
通过训练一个词的后续出现的概率，来推测出现什么词。
涉及概率统计和词序。

# Word bag
词袋模型。
忽略词序，按照词的出现频率构建向量。
通过比较两个向量的余弦相似度来比较。


# Embedding
词向量，通常也叫词嵌入(Word Embedding)，是一种寻找词和词之间相似性的NLP技术，它把词汇各个维度上的特征用数值向量进行表示，利用这些维度上特征的相似程度，就可以判断出哪些词和哪些词语义更接近。


词嵌入：这个术语通常用于描述将**词映射到向量空间**的**过程**或**表示方法**。它通常包括**训练算法和生成的词向量空间**。例如，我们可以说“我们使用**Word2Vec算法**来生成词嵌入”​。

![[Pasted image 20250525110101.png]]

# Word2Vec算法
核心思想：近朱者赤，近墨者黑

时间：2013年 (Tomas Mikolov）和他的Google 同事 开发的。


## 关键概念 稀疏向量、稠密向量

稀疏向量：
* 较高维度，因此：
* 大部分值为0的向量，比如词袋模型中的向量，长度为词汇表的总数。

稠密向量：
* 较低维度，能够捕获丰富的信息
* 大部分值为1的向量，比如word2vec
* 计算效率较高

![[Pasted image 20250525110827.png]]


CBOW 模型：Continuous bag of words 连续词袋模型
Skip-gram 模型：跳词

区别：
* cbow :根据输入词，预测目标词
* skip-gram:根据目标词，预测输入的上下文词

![[Pasted image 20250525111132.png]]



nn.Linear
https://blog.csdn.net/zhaohongfei_358/article/details/122797190

nn.Embedding
https://www.zhihu.com/question/436748480

这个比较好：
https://blog.csdn.net/qq_43391414/article/details/120783887?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-120783887-blog-122797190.235^v43^pc_blog_bottom_relevance_base4&spm=1001.2101.3001.4242.1&utm_relevant_index=2

https://zhuanlan.zhihu.com/p/683843585

比较详细的：https://www.cnblogs.com/chenhuabin/p/17117992.html



创建one-hot的向量
```
word_vector = torch.eye(len(word_dict))  
print(word_vector)



====
tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])
```



